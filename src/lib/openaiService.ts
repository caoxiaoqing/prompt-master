import OpenAI from 'openai';
import { ChatMessage } from '../types';

// OpenAI API æœåŠ¡
export class OpenAIService {
  private static openaiInstances: Map<string, OpenAI> = new Map();

  /**
   * è·å– OpenAI å®¢æˆ·ç«¯å®ä¾‹
   */
  static getClient(baseUrl: string, apiKey: string): OpenAI {
    const instanceKey = `${baseUrl}:${apiKey}`;
    
    if (!this.openaiInstances.has(instanceKey)) {
      console.log('ğŸ”‘ åˆ›å»ºæ–°çš„ OpenAI å®¢æˆ·ç«¯å®ä¾‹:', { baseUrl });
      
      const client = new OpenAI({
        baseURL: baseUrl,
        apiKey: apiKey,
        dangerouslyAllowBrowser: true // å…è®¸åœ¨æµè§ˆå™¨ä¸­ä½¿ç”¨ API å¯†é’¥
      });
      
      this.openaiInstances.set(instanceKey, client);
    }
    
    return this.openaiInstances.get(instanceKey)!;
  }

  /**
   * å‘é€èŠå¤©è¯·æ±‚
   */
  static async sendChatRequest(
    baseUrl: string,
    apiKey: string,
    messages: any[],
    modelName: string,
    temperature: number = 0.7,
    maxTokens: number = 1000,
    topP: number = 1.0,
    topK: number = 50
  ): Promise<{
    content: string;
    tokenUsage: { prompt: number; completion: number; total: number };
    responseTime: number;
  }> {
    try {
      console.log('ğŸš€ å‘é€ OpenAI è¯·æ±‚:', {
        modelName,
        messagesCount: messages.length,
        temperature,
        maxTokens
      });
      
      const startTime = Date.now();
      
      // è·å– OpenAI å®¢æˆ·ç«¯
      const openai = this.getClient(baseUrl, apiKey);
      
      // å‘é€è¯·æ±‚
      const completion = await openai.chat.completions.create({
        messages,
        model: modelName,
        temperature,
        max_tokens: maxTokens,
        top_p: topP,
        top_k: topK
      });
      
      const responseTime = Date.now() - startTime;
      
      console.log('âœ… OpenAI è¯·æ±‚æˆåŠŸ:', {
        responseTime: `${responseTime}ms`,
        hasChoices: completion.choices.length > 0,
        tokenUsage: completion.usage
      });
      
      // æå–å“åº”å†…å®¹
      const content = completion.choices[0]?.message?.content || '';
      
      // è®¡ç®— token ä½¿ç”¨æƒ…å†µ
      const tokenUsage = completion.usage ? {
        prompt: completion.usage.prompt_tokens,
        completion: completion.usage.completion_tokens,
        total: completion.usage.total_tokens
      } : { prompt: 0, completion: 0, total: 0 };
      
      return {
        content,
        tokenUsage,
        responseTime
      };
    } catch (error) {
      console.error('âŒ OpenAI è¯·æ±‚å¤±è´¥:', error);
      
      // æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
      if (error instanceof Error) {
        console.error('é”™è¯¯è¯¦æƒ…:', {
          name: error.name,
          message: error.message,
          stack: error.stack
        });
      }
      
      throw error;
    }
  }

  /**
   * æµ‹è¯•æ¨¡å‹è¿æ¥
   */
  static async testConnection(
    baseUrl: string,
    apiKey: string,
    modelName: string
  ): Promise<boolean> {
    try {
      console.log('ğŸ§ª æµ‹è¯• OpenAI è¿æ¥:', { baseUrl, modelName });
      
      // è·å– OpenAI å®¢æˆ·ç«¯
      const openai = this.getClient(baseUrl, apiKey);
      
      // å‘é€ç®€å•çš„æµ‹è¯•è¯·æ±‚
      const completion = await openai.chat.completions.create({
        messages: [
          { role: 'system', content: 'You are a helpful assistant.' },
          { role: 'user', content: 'Hello, this is a connection test.' }
        ],
        model: modelName,
        max_tokens: 5
      });
      
      console.log('âœ… OpenAI è¿æ¥æµ‹è¯•æˆåŠŸ:', completion.choices[0]?.message?.content);
      
      return true;
    } catch (error) {
      console.error('âŒ OpenAI è¿æ¥æµ‹è¯•å¤±è´¥:', error);
      
      // æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
      if (error instanceof Error) {
        console.error('é”™è¯¯è¯¦æƒ…:', {
          name: error.name,
          message: error.message,
          stack: error.stack
        });
      }
      
      throw error;
    }
  }

  /**
   * å°†èŠå¤©æ¶ˆæ¯è½¬æ¢ä¸º OpenAI API æ ¼å¼
   */
  static convertMessagesToOpenAIFormat(messages: ChatMessage[]): any[] {
    return messages.map(msg => ({
      role: msg.role,
      content: msg.content
    }));
  }
}